# Main Image Simulator Config File
# Created: June 16, 2010
# Authors: Nicole M. Silvestri, Jeff Gardner
#           University of Washington
# Updated: Jan 27, 2012 - JPG
#
# Version: 2.0
# This configuration file is in Windows .ini format for use
#   with the Python ConfigParser class
#
# Note: You should copy/rename this file and edit it for your setup
#________________________________________________________

############################
## GENERAL PARAMETERS
############################

[general]
##
## PYTHON SETUP
##

# We require at least Python 2.5.  Since 2.4 is the default on many systems,
# include the name/location of a Python 2.5 or greater.  This will be
# called during the preprocessing stage to execute "fullFocalPlane.py"
# and "chip.py"
python-exec: python

## SCHEDULER SELECTION
#
# It is possible to use different schedulers for different processing
# phases.  For example, one can select "csh" to generate a regular
# shell script for the pre-processing, but then have this generate
# a set of PBS batch jobs.

# The scheduler used for the pre-processing phase:
#    csh, pbs, or exacycle
scheduler1: pbs

# The scheduler used for the per-chip phase (raytracing and beyond):
#    csh, pbs, or exacycle
scheduler2: pbs


## JOB PARAMETERS

# Number of nodes per job
numNodes: 1

# Number of processors per job (eg: 1-8)
processors: 1

# Processor memory in MB
pmem: 1000

# Job name (eg: username on the cluster)
jobname: JeffsImSim

# Jobs can optionally sleep for a random number of seconds between
# 0 and 'sleepmax' upon startup.  This prevents jobs starting at the exact
# same time on a specific execution node and thus stepping on top of
# one another when they are trying to determine if the scratchDataDir
# is already present and intact.
# Setting sleepmax to '60' on Minerva yields good results.
# Setting sleepmax to '0' will disable this feature.
sleepmax: 0

# The level of debugging information ('0'=none, '1' adds '-x' to shell scripts)
debuglevel: 0

##
## DIRECTORY & PATH SETUP
##
# There are 3 main storage locations: 
#    1. On the submission node (i.e. the client node),
#    2. On a shared storage volume that is accessible from both the submit
#           and the execution nodes.
#    3. On the execution nodes.
#  The directory structure in each location is as follows:
#    
#     submission:
#        /"IMSIM_HOME_PATH"               # Absolute path to ImSim directory tree
#        /"IMSIM_EXEC_PATH"               # If this is defined, it is the location of the
#                                         # ImSim executables (for build systems that place
#                                         # executables in different locations).  If not 
#                                         # defined, executables are assumed to be in IMSIM_HOME_PATH.
#                                         # Note that IMSIM_EXEC_PATH must still preserve the 
#                                         # same directory structure as IMSIM_HOME_PATH.
#     shared:
#        /"CAT_SHARE_DATA"/"dataTarball"  # Abs. path to the tarball containing catalog input data
#
#        /"stagePath1"                    # Abs. path to which files are staged before execution
#                                         # of preprocessing phase
#        /"stagePath1"/trimfiles          # Staging area for trimfiles
#        /"stagePath1"/*_f[filter].[pbs,csh] # Preprocessing scripts
#        /"stagePath1"/visitFiles*-fr.tar.gz # Per-visit files for preprocessing step.
#        /"stagePath1"/imsimExecFiles.tar.gz # Exec files needed for all preprocessing visits
#
#        /"stagePath2"                    # Absolute path for files output by preprocessing
#                                         # step and staging for raytracing step.
#        /"stagePath2"/*-f[filter]        # Files for each full focal plane visit
#        /"stagePath2"/*-f[filter]/nodefiles*.tar.gz  # Files for this visit common to all detectors
#        /"stagePath2"/*-f[filter]/run*   # Param files for each detector/exposure and atmosphere screens
#  
#        /"savePath"                      # Abs. path to which output data is written
#        /"savePath"/*-f[filter]/logs     # log files organized by full focal plane visit
#        /"savePath"/imSim                # Output images! (Yes, eventually we do generate these!)
#     execution:
#        /"scratchSharedPath"             # Abs path to the location of the untarred shared catalog data
#                                         # (Not including the final "data" directory)
#        /"scratchExecPath"               # Abs. path to execution directory on compute node
#        /"scratchExecPath"/<workunitID>  # Path to a specific work unit's ImSim directory tree on the exec node
#        /"scratchExecPath"/<workunitID>/"scratchOutputDir" # Path to output data on the exec node
#
#     <workunitID> is determined at runtime:
#         e.g. obshistid_id"f"filter or "1111110_R01_S00_E000fr"
#
# Notes: 
#    - stagePath1, stagePath2, and savePath can all point to the same location if desired.
#    - scratchSharedPath and scratchExecPath can also point to same location if desired.
#           

## Use the following two environment variables to define absolute paths:
#setenv IMSIM_HOME_DIR /share/home/nms/pt1.2imsimTrunk/
#setenv CAT_SHARE_DATA /share/pogo3/krughoff/shared/

# Name of the tarball containing the input data (SEDS, QE, and height maps)
# in the client-side 'CAT_SHARE_DATA' directory.
dataTarball: data.tar.gz

# Absolute path to the staging directory for the preprocessing stage
# This path must be visible from both the submit and execution nodes.
stagePath1: /share/lsstpoly/gardnerj/staging1

# Absolute path to the staging directory for the raytracing stage.
# This path must be visible from the execution nodes that run the
# preprocessing stage and execution nodes that run the raytracing stage.
# (Note: this can be set to the same location as stagePath1 if desired)
stagePath2: /share/lsstpoly/gardnerj/staging2

# Absolute path to the directory where the final images/logs are to be written
# This path must be visible from both the submit and execution nodes.
# (Note: This can be set to the same location as either or both of
#  the stagePaths if desired)
savePath: /share/lsstpoly/gardnerj/output

# Absolute path from the scratch directory to the directory that has
# the SEDS, QE, and height maps.  There is no harm in setting this
# to the same value as scratchExecPath.
scratchSharedPath: /state/partition1

# Absolute path for scratch partition where the jobs will be run.
# If this is being run in a cluster environment, this will be the
# directory on the remote nodes.  There is no harm is setting this
# (For Minerva and other HPC systems, this should include one's username)
scratchExecPath: /state/partition1/gardnerj

# Name of temporary directory within /"scratchExecPath"/<workunitID>
# to store images. This is also the location for the trimfiles for each run.
scratchOutputDir: simOutput



## JOB MONITOR DATABASE

# Use the jobAllocator database (true or false)
# If "True" you will be required to enter lsst or exacycle-specific database params
# Note: Use "True" or "False" and note that these are case sensitive!
useDatabase: False

############################
## PBS-SPECIFIC PARAMETERS
############################

[pbs]
## PBS JOB PARAMS

# Cluster queue (eg: default, debug, scavenge, astro)
queue: default 

# Maximum time in hh:mm:ss that each job will run on the cluster (eg: 08:00:00)
walltime: 12:00:00

# Cluster user name 
username: gardnerj

# Root email address for notifications about cluster jobs (eg: @email_address)
rootEmail: @phys.washington.edu


## PBS JOB DIRECTORY & PATH SETUP

# JPG: Look like this is not actually used
# Tempoprary landing path for all files created for each imsim run (eg. parameter, fits files)
imsimRunDir: parFilesTemp

## SUBMIT (QSUB) SCRIPT SETUP

# Time in seconds for script to sleep between queue queries
sleep: 300

# Time in seconds to wait between successive job submissions to the cluster
wait: 2

# Maximum number of jobs allowed to be queued (running + idle) on the cluster
maxJobs: 10

############################
## LSST-SPECIFIC PARAMETERS
############################
## (These options require the LSST stack)

[lsst]
## JOB MONITOR DATABASE SETUP

# JobAllocator database table number (50-100)
tableId: 75

# sims/catalogs/generation location
catGen: /share/home/nms/catalogs_generation 
 
