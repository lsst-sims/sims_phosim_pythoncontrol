######################################################################
# Main Image Simulator Config File
#
# Please read the "CONFIGURATION FILES" section of README.txt
# for an explanation of this file.
#
# Note: You should copy/rename this file and edit it for your setup
######################################################################

############################
## GENERAL PARAMETERS
############################

[general]
##
## PYTHON SETUP
##

# We require at least Python 2.5.  Since 2.4 is the default on many systems,
# include the name/location of a Python 2.5 or greater.  This will be
# called during the preprocessing stage to execute "fullFocalPlane.py"
# and "chip.py"
python-exec: python

## SCHEDULER SELECTION
#
# It is possible to use different schedulers for different processing
# phases.  For example, one can select "csh" to generate a regular
# shell script for the pre-processing, but then have this generate
# a set of PBS batch jobs.

# The scheduler used for the pre-processing phase:
#    csh, pbs, or exacycle
scheduler1: pbs

# The scheduler used for the per-chip phase (raytracing and beyond):
#    csh, pbs, or exacycle
scheduler2: pbs


## JOB PARAMETERS

# Number of nodes per job
numNodes: 1

# Number of processors per job (eg: 1-8)
processors: 8

# Processor memory in MB
pmem: 1000

# Job name (eg: username on the cluster)
jobname: JeffsImSim

# Jobs can optionally sleep for a random number of seconds between
# 0 and 'sleepmax' upon startup.  This prevents jobs starting at the exact
# same time on a specific execution node and thus stepping on top of
# one another when they are trying to determine if the scratchDataDir
# is already present and intact.
# Setting sleepmax to '60' on Minerva yields good results.
# Setting sleepmax to '0' will disable this feature.
sleepmax: 0

# The level of debugging information ('0'=none, '1' adds '-x' to shell scripts)
debuglevel: 1


##
## DIRECTORY & PATH SETUP
##

# The directory and path setup is explained in detail in README.txt

##
## Shared input datasets:
## ----------------------

# The shared input data is divided into two parts.  The first set contains the
# data needed for the preprocessing stage (QE and height maps).  The
# second set contains the SEDs needed for the raytracing stage.
#
#!!!!!!!!!!!!!!!!
# IMPORTANT NOTE: Unlike with the "full_focalplane" shell script, the data
#!!!!!!!!!!!!!!!! tarball used here should *not* contain "data" as the root
#                 directory (this just made things too complicated in the script
#                 logic).  For example, the directories "focal_plane," agnSED," starSED,"
#                 etc should be in the root of the tarball.
# 
# In the case of the either dataset, the tarball
# is copied from shared storage to scratchSharedPath and untarred
# UNLESS useSharedFP or useSharedSEDs is set to "true".  In
# this case, the data will be read directly from shared storage instead.
# Note that to read directly from shared storage, it must be from
# a filesystem that supports symbolic links.

# true:  Read preprocessing shared data directly from shared storage location
# false: Copy the dataTarballFP to local storage ("scratchDataPath") and untar
useSharedFP: false

# If useSharedFP is "true":  Ignore this parameter
# If useSharedFP is "false": Name of "FP" data tarball to copy to "scratchDataPath"
dataTarballFP: data_02152012.tar

# Path to shared "FP" data.  This will either be
# If useSharedFP is "true":  The path that contains the focal_plane directory
# If useSharedFP is "false": The path that contains the shared data tarball
#                             "<dataTarballFP> (i.e. "<dataPathFP>"/"<dataTarballFP>")
dataPathFP: /share/lsstpoly/gardnerj/data

# Absolute path on the execution node to which the shared data will be staged.
# This should be different from scratchExecPath or else the shared data will
# be deleted from the exec node upon completion of this work unit. 
# The resulting untarred data will be stored in "scratchDataPath".
# If useShared[FP,SEDs] is set to "true" this is ignored (since no data is staged)
scratchDataPathFP: /state/partition1/shared/lsst

# true:  Read preprocessing shared data directly from shared storage location
# false: Copy the dataTarballSEDs to local storage ("scratchDataPath") and untar
useSharedSEDs: false

# If useSharedSEDs is "true":  Ignore this parameter
# If useSharedSEDs is "false": Name of "PRE" data tarball to copy to "scratchDataPath"
dataTarballSEDs: data_02152012.tar

# Path to shared "SEDs" data.  This will either be
# If useSharedSEDs is "true":  The path that contains the focal_plane and *SED directories
# If useSharedSEDs is "false": The path that contains the shared data tarball "<dataTarballSEDs>"
#                              (i.e. "<dataPathSEDs>/<dataTarballSEDs>")
dataPathSEDs: /share/lsstpoly/gardnerj/data

# Absolute path on the execution node to which the shared data will be staged.
# This should be different from scratchExecPath or else the shared data will
# be deleted from the exec node upon completion of this work unit. 
# The resulting untarred data will be stored in "scratchDataPath/sharedData".
# If useShared[PRE,SEDs] is set to "true" this is ignored (since no data is staged)
scratchDataPathSEDs: /state/partition1/shared/lsst

##
## Paths for work-specific data and parameter files
##

# Absolute path to the staging directory for the preprocessing stage
# This path must be visible from both the submit and execution nodes.
stagePath1: /share/lsstpoly/gardnerj/shared/staging1

# Absolute path to the staging directory for the raytracing stage.
# This path must be visible from the execution nodes that run the
# preprocessing stage and execution nodes that run the raytracing stage.
# (Note: this can be set to the same location as stagePath1 if desired)
stagePath2: /share/lsstpoly/gardnerj/shared/staging2

# Absolute path to the directory where the final images/logs are to be written
# This path must be visible from both the submit and execution nodes.
# (Note: This can be set to the same location as either or both of
#  the stagePaths if desired)
savePath: /share/lsstpoly/gardnerj/shared/output

# Absolute path for scratch partition where the jobs will be run.
# If this is being run in a cluster environment, this will be the
# directory on the remote nodes.  There is no harm is setting this
# (For Minerva and other HPC systems, this should include one's username)
scratchExecPath: /state/partition1/gardnerj

# Name of temporary directory within /"scratchExecPath"/<workunitID>
# to store images. This is also the location for the trimfiles for each run.
scratchOutputDir: simOutput



## JOB MONITOR DATABASE

# Use the jobAllocator database (true or false)
# If "True" you will be required to enter lsst or exacycle-specific database params
# Note: Use "True" or "False" and note that these are case sensitive!
useDatabase: False

############################
## PBS-SPECIFIC PARAMETERS
############################

[pbs]
## PBS JOB PARAMS

# Cluster queue (eg: default, debug, scavenge, astro)
queue: default 

# Maximum time in hh:mm:ss that each job will run on the cluster (eg: 08:00:00)
walltime: 12:00:00

# Cluster user name 
username: gardnerj

# Root email address for notifications about cluster jobs (eg: @email_address)
rootEmail: @phys.washington.edu


## PBS JOB DIRECTORY & PATH SETUP

# JPG: Look like this is not actually used
# Tempoprary landing path for all files created for each imsim run (eg. parameter, fits files)
imsimRunDir: parFilesTemp

## SUBMIT (QSUB) SCRIPT SETUP

# Time in seconds for script to sleep between queue queries
sleep: 300

# Time in seconds to wait between successive job submissions to the cluster
wait: 2

# Maximum number of jobs allowed to be queued (running + idle) on the cluster
maxJobs: 10

############################
## LSST-SPECIFIC PARAMETERS
############################
## (These options require the LSST stack)

[lsst]
## JOB MONITOR DATABASE SETUP

# JobAllocator database table number (50-100)
tableId: 75

# sims/catalogs/generation location
catGen: /share/home/nms/catalogs_generation 
 
