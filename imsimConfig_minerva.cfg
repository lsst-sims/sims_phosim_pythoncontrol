# Main Image Simulator Config File
# Created: June 16, 2010
# Authors: Nicole M. Silvestri, Jeff Gardner
#           University of Washington
# Updated: Jan 27, 2012 - JPG
#
# Version: 2.0
# This configuration file is in Windows .ini format for use
#   with the Python ConfigParser class
#
# Note: You should copy/rename this file and edit it for your setup
#________________________________________________________

############################
## GENERAL PARAMETERS
############################

[general]
##
## PYTHON SETUP
##

# We require at least Python 2.5.  Since 2.4 is the default on many systems,
# include the name/location of a Python 2.5 or greater.  This will be
# called during the preprocessing stage to execute "fullFocalPlane.py"
# and "chip.py"
python-exec: python

## SCHEDULER SELECTION
#
# It is possible to use different schedulers for different processing
# phases.  For example, one can select "csh" to generate a regular
# shell script for the pre-processing, but then have this generate
# a set of PBS batch jobs.

# The scheduler used for the pre-processing phase:
#    csh, pbs, or exacycle
scheduler1: pbs

# The scheduler used for the per-chip phase (raytracing and beyond):
#    csh, pbs, or exacycle
scheduler2: pbs


## JOB PARAMETERS

# Number of nodes per job
numNodes: 1

# Number of processors per job (eg: 1-8)
processors: 8

# Processor memory in MB
pmem: 1000

# Job name (eg: username on the cluster)
jobname: JeffsImSim

# Jobs can optionally sleep for a random number of seconds between
# 0 and 'sleepmax' upon startup.  This prevents jobs starting at the exact
# same time on a specific execution node and thus stepping on top of
# one another when they are trying to determine if the scratchDataDir
# is already present and intact.
# Setting sleepmax to '60' on Minerva yields good results.
# Setting sleepmax to '0' will disable this feature.
sleepmax: 0

# The level of debugging information ('0'=none, '1' adds '-x' to shell scripts)
debuglevel: 1


##
## DIRECTORY & PATH SETUP
##
# There are 3 main storage locations: 
#    1. On the submission node (i.e. the client node),
#    2. On a shared storage volume that is accessible from both the submit
#           and the execution nodes.
#    3. On the execution nodes.
#  The directory structure in each location is as follows:
#    
#     submission (visible only from submission node):
#        /"IMSIM_SOURCE_PATH"             # Absolute path to ImSim source directory tree
#        /"IMSIM_EXEC_PATH"               # If this is defined, it is the location of the
#                                         # ImSim executables (for build systems that place
#                                         # executables in different locations).  If not 
#                                         # defined, executables are assumed to be in IMSIM_HOME_PATH.
#                                         # Note that IMSIM_EXEC_PATH must still preserve the 
#                                         # same directory structure as IMSIM_HOME_PATH.
#     shared storage (visible from both submission and execution nodes):
#        /"dataPathPRE"/"dataTarballPRE"  # Abs. path to the "preprocessing" tarball containing
#                                         # QE and height maps (if useSharedPRE is "false")
#        /"dataPathPRE"                   # Abs. path to the "preprocessing" data directory structure
#                                         # containing QE and heigh height maps (if useSharedPRE is "true")
#        /"dataPathSEDs"/"dataTarballSEDs"# Abs. path to the "main" tarball containing SEDs
#        /"dataPathSEDs"                  # Abs. path to the "main" data directory structure containing
#                                         # SEDs (if useSharedSEDs is "true")
#
#        /"stagePath1"                    # Abs. path to which files are staged before execution
#                                         # of preprocessing phase
#        /"stagePath1"/trimfiles          # Staging area for trimfiles
#        /"stagePath1"/*_f[filter].[pbs,csh] # Preprocessing scripts
#        /"stagePath1"/visitFiles*-fr.tar.gz # Per-visit files for preprocessing step.
#        /"stagePath1"/imsimExecFiles.tar.gz # Exec files needed for all preprocessing visits
#
#        /"stagePath2"                    # Absolute path for files output by preprocessing
#                                         # step and staging for raytracing step.
#        /"stagePath2"/*-f[filter]        # Files for each full focal plane visit
#        /"stagePath2"/*-f[filter]/nodefiles*.tar.gz  # Files for this visit common to all detectors
#        /"stagePath2"/*-f[filter]/run*   # Param files for each detector/exposure and atmosphere screens
#  
#        /"savePath"                      # Abs. path to which output data is written
#        /"savePath"/*-f[filter]/logs     # log files organized by full focal plane visit
#        /"savePath"/imSim                # Output images! (Yes, eventually we do generate these!)
#
#     execution (visible only from execution node):
#        /"scratchSharedPath"             # Abs path to the location of the untarred shared catalog data
#        /"scratchExecPath"               # Abs. path to execution directory on compute node
#        /"scratchExecPath"/<workunitID>  # Path to a specific work unit's ImSim directory tree on the exec node
#        /"scratchExecPath"/<workunitID>/"scratchOutputDir" # Path to output data on the exec node
#
#     <workunitID> is determined at runtime and is of the form:
#          <obshistid>-f<filter>-<id> where:
#                obshistid:  obshistid including extraID
#                filter:     filter letter ID
#                id:         full exposure id of the form R[0-4][0-4]_S[0-2][0-2]_E[0-9][0-9][0-9]
#          Example: "1111110-fr-R01_S12_E001"
#
# Simplifications: 
#    - stagePath1, stagePath2, and savePath can all point to the same location if desired.
#    - scratchSharedPath and scratchExecPath can also point to same location if desired.
#    - dataTarballPRE and dataTarballSEDs can be the same tarball
#           

## You must have the IMSIM_SOURCE_PATH environment variable defined. 
## It is the location of the ImSim source tree.  If the ImSim
## binaries are also included in the source tree path, then
## This is the only environment variable you need to define
# Example:
#    setenv IMSIM_SOURCE_PATH /local/gardnerj/lsst/imsims/v-2.2.1

## If the ImSim binaries are in a location separate from the source
## tree, then you need to define IMSIM_EXEC_PATH as well
# Example:
#    setenv IMSIM_EXEC_PATH /local/gardnerj/lsst/bin

##
## Shared input datasets:
## ----------------------

# The shared input data is divided into two parts.  The first set contains the
# data needed for the preprocessing stage (QE and height maps).  The
# second set contains the SEDs needed for the raytracing stage.
#
#!!!!!!!!!!!!!!!!
# IMPORTANT NOTE: Unlike with the "full_focalplane" shell script, the data
#!!!!!!!!!!!!!!!! tarball used here should *not* contain "data" as the root
#                 directory (this just made things too complicated in the script
#                 logic).  For example, the directories "focal_plane," agnSED," starSED,"
#                 etc should be in the root of the tarball.
# 
# In the case of the either dataset, the tarball
# is copied from shared storage to scratchSharedPath and untarred
# UNLESS useSharedPRE or useSharedSEDs is set to "true".  In
# this case, the data will be read directly from shared storage instead.
# Note that to read directly from shared storage, it must be from
# a filesystem that supports symbolic links.

# true:  Read preprocessing shared data directly from shared storage location
# false: Copy the dataTarballPRE to local storage ("scratchDataPath") and untar
useSharedPRE: false

# If useSharedPRE is "true":  Ignore this parameter
# If useSharedPRE is "false": Name of "PRE" data tarball to copy to "scratchDataPath"
dataTarballPRE: data_02152012.tar

# Path to shared "PRE" data.  This will either be
# If useSharedPRE is "true":  The path that contains the focal_plane directory
# If useSharedPRE is "false": The path that contains the shared data tarball
#                             "<dataTarballPRE> (i.e. "<dataPathPRE>"/"<dataTarballPRE>")
dataPathPRE: /share/lsstpoly/gardnerj/data

# Absolute path on the execution node to which the shared data will be staged.
# This should be different from scratchExecPath or else the shared data will
# be deleted from the exec node upon completion of this work unit. 
# The resulting untarred data will be stored in "scratchDataPath".
# If useShared[PRE,SEDs] is set to "true" this is ignored (since no data is staged)
scratchDataPathPRE: /state/partition1/shared/lsst

# true:  Read preprocessing shared data directly from shared storage location
# false: Copy the dataTarballPRE to local storage ("scratchDataPath") and untar
useSharedSEDs: false

# If useSharedSEDs is "true":  Ignore this parameter
# If useSharedSEDs is "false": Name of "PRE" data tarball to copy to "scratchDataPath"
dataTarballSEDs: data_02152012.tar

# Path to shared "SEDs" data.  This will either be
# If useSharedSEDs is "true":  The path that contains the focal_plane and *SED directories
# If useSharedSEDs is "false": The path that contains the shared data tarball "<dataTarballSEDs>"
#                              (i.e. "<dataPathSEDs>/<dataTarballSEDs>")
dataPathSEDs: /share/lsstpoly/gardnerj/data

# Absolute path on the execution node to which the shared data will be staged.
# This should be different from scratchExecPath or else the shared data will
# be deleted from the exec node upon completion of this work unit. 
# The resulting untarred data will be stored in "scratchDataPath/sharedData".
# If useShared[PRE,SEDs] is set to "true" this is ignored (since no data is staged)
scratchDataPathSEDs: /state/partition1/shared/lsst

##
## Paths for work-specific data and parameter files
##

# Absolute path to the staging directory for the preprocessing stage
# This path must be visible from both the submit and execution nodes.
stagePath1: /share/lsstpoly/gardnerj/shared/staging1

# Absolute path to the staging directory for the raytracing stage.
# This path must be visible from the execution nodes that run the
# preprocessing stage and execution nodes that run the raytracing stage.
# (Note: this can be set to the same location as stagePath1 if desired)
stagePath2: /share/lsstpoly/gardnerj/shared/staging2

# Absolute path to the directory where the final images/logs are to be written
# This path must be visible from both the submit and execution nodes.
# (Note: This can be set to the same location as either or both of
#  the stagePaths if desired)
savePath: /share/lsstpoly/gardnerj/shared/output

# Absolute path for scratch partition where the jobs will be run.
# If this is being run in a cluster environment, this will be the
# directory on the remote nodes.  There is no harm is setting this
# (For Minerva and other HPC systems, this should include one's username)
scratchExecPath: /state/partition1/gardnerj

# Name of temporary directory within /"scratchExecPath"/<workunitID>
# to store images. This is also the location for the trimfiles for each run.
scratchOutputDir: simOutput



## JOB MONITOR DATABASE

# Use the jobAllocator database (true or false)
# If "True" you will be required to enter lsst or exacycle-specific database params
# Note: Use "True" or "False" and note that these are case sensitive!
useDatabase: False

############################
## PBS-SPECIFIC PARAMETERS
############################

[pbs]
## PBS JOB PARAMS

# Cluster queue (eg: default, debug, scavenge, astro)
queue: default 

# Maximum time in hh:mm:ss that each job will run on the cluster (eg: 08:00:00)
walltime: 12:00:00

# Cluster user name 
username: gardnerj

# Root email address for notifications about cluster jobs (eg: @email_address)
rootEmail: @phys.washington.edu


## PBS JOB DIRECTORY & PATH SETUP

# JPG: Look like this is not actually used
# Tempoprary landing path for all files created for each imsim run (eg. parameter, fits files)
imsimRunDir: parFilesTemp

## SUBMIT (QSUB) SCRIPT SETUP

# Time in seconds for script to sleep between queue queries
sleep: 300

# Time in seconds to wait between successive job submissions to the cluster
wait: 2

# Maximum number of jobs allowed to be queued (running + idle) on the cluster
maxJobs: 10

############################
## LSST-SPECIFIC PARAMETERS
############################
## (These options require the LSST stack)

[lsst]
## JOB MONITOR DATABASE SETUP

# JobAllocator database table number (50-100)
tableId: 75

# sims/catalogs/generation location
catGen: /share/home/nms/catalogs_generation 
 
