#<?cfg paf policy ?>

# Main Image Simulator Policy File
# Created: June 16, 2010
# Author:  Nicole M. Silvestri, University of Washington
#	   gardnerj@phys.washington.edu
# Updated: Nov 30, 2011 - JPG
#
# Version: 2.0
# This configuration file is in Windows .ini format for use
#   with the Python ConfigParser class
#
# Note: You should copy/rename this file and edit it for your setup
#________________________________________________________

############################
## GENERAL PARAMETERS
############################

[general]
## SCHEDULER SELECTION
#
# It is possible to use different schedulers for different processing
# phases.  For example, one can select "csh" to generate a regular
# shell script for the pre-processing, but then have this generate
# a set of PBS batch jobs.

# The scheduler used for the pre-processing phase:
#    csh, pbs, or exacycle
scheduler1: pbs

# The scheduler used for the per-chip phase (raytracing and beyond):
#    csh, pbs, or exacycle
scheduler2: pbs


## JOB PARAMETERS

# Number of nodes per job
numNodes: 1

# Number of processors per job (eg: 1-8)
processors: 1

# Processor memory in MB
pmem: 1000

# Job name (eg: username on the cluster)
jobname: JeffsImSim

# Jobs can optionally sleep for a random number of seconds between
# 0 and 'sleepmax' upon startup.  This prevents jobs starting at the exact
# same time on a specific execution node and thus stepping on top of
# one another when they are trying to determine if the scratchDataDir
# is already present and intact.
# Setting sleepmax to '60' on Minerva yields good results.
# Setting sleepmax to '0' will disable this feature.
sleepmax: 0

# The level of debugging information ('0'=none, '1' adds '-x' to shell scripts)
debuglevel: 0

##
## DIRECTORY & PATH SETUP
##
# There are 3 main storage locations: 
#    1. On the submission node (i.e. the client node),
#    2. On a shared storage volume that is accessible from both the submit
#           and the execution nodes.
#    3. On the execution nodes.
#  The directory structure in each location is as follows:
#    
#     submission:
#        /"IMSIM_HOME_DIR"                # Absolute path to ImSim directory tree
#     shared:
#        /"CAT_SHARE_DATA"/"dataTarball"  # Abs. path to the tarball containin catalog input data
#
#        /"stagingPath1"                  # Abs. path to which files are staged before execution
#                                         # of preprocessing phase
#        /"stagingPath1"/trimfiles        # Staging area for trimfiles
#        /"stagingPath1"/*_f[filter].[pbs,csh] # Preprocessing scripts
#        /"stagingPath1"/visitFiles*-fr.tar.gz # Per-visit files for preprocessing step.
#        /"stagingPath1"/imsimExecFiles.tar.gz # Exec files needed for all preprocessing visits
#
#        /"stagingPath2"                  # Absolute path for files output by preprocessing
#                                         # step and stages for raytracing step.
#        /"stagingPath2"/*-f[filter]      # Files for each full focal plane visit
#        /"stagingPath2"/*-f[filter]/"nodefiles*.tar.gz"  # Files for this visit common to all detectors
#        /"stagingPath2"/*-f[filter]/run* # Param files for each detector/exposure and atmosphere screens
#  
#        /"savePath"                      # Abs. path to which output data is written
#        /"savePath"/*-f[filter]/logs     # log files organized by full focal plane visit
#        /"savePath"/imSim                # Output images! (Yes, eventually we do generate these!)
#     execution:
#        /"scratchPath"                   # Abs. path to scratch directory on execution node
#        /"scratchPath"/"scratchDataDir"  # Path to the location of the untarred catalog data
#        /"scratchPath"/<scratchExecDir>  # Path to a specific work unit's ImSim directory tree on the exec node
#        /"scratchPath"/<scratchExecDir>/"scratchOutputDir" # Path to output data on the exec node
#
#     <scratchExecDir> is determined at runtime and is the name of the work unit
#         (e.g. obshistid_id"f"filter or "1111110_R01_S00_E000fr")
#           

## Use the following two environment variables to define absolute paths:
#setenv IMSIM_HOME_DIR /share/home/nms/pt1.2imsimTrunk/
#setenv CAT_SHARE_DATA /share/pogo3/krughoff/shared/

# Absolute path to the directory where the final images/logs are to be written
# This path must be visible from both the submit and execution nodes.
# (Note, the files needed for each of the single-CCD runs are staged here after the
#  preprocessing runs).
savePath: /share/lsstpoly/gardnerj/output

# Absolute path to the staging directory for the preprocessing stage
# This path must be visible from both the submit and execution nodes.
stagingPath1: /share/lsstpoly/gardnerj/staging1

# Absolute path to the staging directory for the raytracing stage.
# This path must be visible from the execution nodes that run the
# preprocessing stage and execution nodes that run the raytracing stage.
stagingPath2: /share/lsstpoly/gardnerj/staging2

# Name of the tarball containing the input data (SEDS, QE, and height maps)
# in the client-side 'CAT_SHARE_DATA' directory.
dataTarball: data.tar.gz

# Absolute path for scratch partition where the jobs will be run.
# If this is being run in a cluster environment, this will be the
# directory on the remote nodes.
# (For PBS, the actual scratch directory will be 'scratchPath/$USER')
scratchPath: /state/partition1

# Name of temporary directory within the  to store images.
# This is also the location for the trimfiles for each run.
# Note: for PBS, this directory will be created in scratchPath/$USER
scratchOutputDir: simOutput

# Relative path from the scratch directory to the directory that has
# the SEDS, QE, and height maps
#datadir: data
scratchDataDir: data



## JOB MONITOR DATABASE

# Use the jobAllocator database (true or false)
# If "True" you will be required to enter lsst or exacycle-specific database params
# Note: Use "True" or "False" and note that these are case sensitive!
useDatabase: False

############################
## PBS-SPECIFIC PARAMETERS
############################

[pbs]
## PBS JOB PARAMS

# Cluster queue (eg: default, debug, scavenge, astro)
queue: default 

# Maximum time in hh:mm:ss that each job will run on the cluster (eg: 08:00:00)
walltime: 12:00:00

# Cluster user name 
username: gardnerj

# Root email address for notifications about cluster jobs (eg: @email_address)
rootEmail: @phys.washington.edu


## PBS JOB DIRECTORY & PATH SETUP

# JPG: Look like this is not actually used
# Tempoprary landing path for all files created for each imsim run (eg. parameter, fits files)
imsimRunDir: parFilesTemp

## SUBMIT (QSUB) SCRIPT SETUP

# Time in seconds for script to sleep between queue queries
sleep: 300

# Time in seconds to wait between successive job submissions to the cluster
wait: 2

# Maximum number of jobs allowed to be queued (running + idle) on the cluster
maxJobs: 10

############################
## LSST-SPECIFIC PARAMETERS
############################
## (These options require the LSST stack)

[lsst]
## JOB MONITOR DATABASE SETUP

# JobAllocator database table number (50-100)
tableId: 75

# sims/catalogs/generation location
catGen: /share/home/nms/catalogs_generation 
 
